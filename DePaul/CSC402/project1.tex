\documentclass[justified,sixbynine]{tufte-book}

\title{\LARGE Annotated Algorithms in Python}
\subtitle{with applications in Physics, Biology, and Finance}

\author{Massimo Di Pierro}
\publisher{Experts4Solutions}

% For nicely typeset tabular material
\usepackage{color}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{tocloft}
\usepackage{parskip}
\usepackage{upquote}
\usepackage{verbatim}
\usepackage{listings}
\usepackage{url}
\usepackage[utf8x]{inputenc}

\sloppy
\makeindex

\def\stackunder#1#2{\mathrel{\mathop{#2}\limits_{#1}}}
\definecolor{lg}{rgb}{0.9,0.9,0.9}
\definecolor{dg}{rgb}{0.3,0.3,0.3}
\def\ft{\small\tt}
\def\func{\textrm}
\def\subsubsection#1{{\bf #1}}

\theoremstyle{plain}% default
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemmma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem{case}{Case}

\lstset{language=Python,
   breaklines=true, basicstyle=\ttfamily\color{black}\footnotesize,
   keywordstyle=\bf\ttfamily,
   commentstyle=\it\ttfamily,
   stringstyle=\color{dg}\it\ttfamily,
   numbers=left, numberstyle=\color{dg}\tiny, stepnumber=1, numbersep=5pt,
   % frame=lr,
   backgroundcolor=\color{lg},
   tabsize=4, showspaces=false,
   showstringspaces=false,
   aboveskip=6pt,
   belowskip=-3pt
}

% Disable Tufte-style captions for ctables
\makeatletter % allows @ in macro names
\def\@ctblCaption{
   \ifx\@ctblcap\undefined\let\@ctblcap\@ctblcaption\fi
   \ifx\@ctblcaption\empty\else
      \gdef\@ctblcaptionarg{\ifx\@ctbllabel\empty\else\label{\@ctbllabel}\fi
         \@ctblcaption\ \@ctblcontinued\strut}
      \ifx\@ctblcap\empty
         \caption[]{\@ctblcaptionarg}
      \else
         \caption[\@ctblcap]{\@ctblcaptionarg}
      \fi
   \fi
}
\makeatother % restores original meaning of @

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
% Generates the index
\begin{document}

\frontmatter

\maketitle

\thispagestyle{empty}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2mm}
{\footnotesize
\vskip 1in
Copyright 2013 by Massimo Di Pierro. All rights reserved.
\vskip 1cm

THE CONTENT OF THIS BOOK IS PROVIDED UNDER THE TERMS OF THE CREATIVE COMMONS PUBLIC LICENSE BY-NC-ND 3.0.

\url{http://creativecommons.org/licenses/by-nc-nd/3.0/legalcode}

THE WORK IS PROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.

BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. TO THE EXTENT THIS LICENSE MAY BE CONSIDERED TO BE A CONTRACT, THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.

Limit of Liability/Disclaimer of Warranty: While the publisher and
author have used their best efforts in preparing this book, they
make no representations or warranties with respect to the accuracy
or completeness of the contents of this book and specifically
disclaim any implied warranties of merchantability or fitness for a
particular purpose.  No warranty may be created or extended by
sales representatives or written sales materials.
The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional
where appropriate.  Neither the publisher nor the author shall be liable
for any loss of profit or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages. \\ \\

For more information about appropriate use of this material, contact:

\begin{verbatim}
Massimo Di Pierro
School of Computing
DePaul University
243 S Wabash Ave
Chicago, IL 60604 (USA)
Email: massimo.dipierro@gmail.com
\end{verbatim}

Library of Congress Cataloging-in-Publication Data: \\ \\
Build Date: \today
}


\newpage
%\begin{center}
%\noindent\fontsize{12}{18}\selectfont\itshape
%\nohyphenation
\thispagestyle{empty}
\phantom{placeholder}
\vspace{2in}
\hskip 3in
{\it to my parents}
%\end{center}
\newpage
\thispagestyle{empty}
\phantom {a}
\newpage

\setlength{\cftparskip}{\baselineskip}
\tableofcontents

\mainmatter
\begin{fullwidth}

\goodbreak\chapter{Introduction}

\goodbreak\chapter{Overview of the Python Language}

\chapter{Theory of Algorithms}

An algorithm is a step-by-step procedure for solving a problem and is typically developed before doing any programming.
The word comes from {\it algorism}, from the mathematician al-Khwarizmi, and was used to refer to the rules of performing arithmetic using Hindu--Arabic numerals and the systematic solution of equations.

In fact, algorithms are independent
of any programming language. Efficient algorithms can have a dramatic effect
on our problem-solving capabilities.

The basic steps of algorithms are loops ({\ft for}, conditionals ({\ft if}), and function calls.
Algorithms also make use of arithmetic expressions, logical expressions ({\ft not}, {\ft and}, {\ft or}), and expressions that can be reduced to the other basic components.

The issues that concern us when developing and analyzing algorithms are the following:

\begin{enumerate}
\item  Correctness: of the problem specification, of the proposed algorithm,
and of its implementation in some programming language (we will not worry
about the third one; program verification is another subject altogether)

\item  Amount of work done: for example, running time of the algorithm in terms of
the input size (independent of hardware and programming language)

\item  Amount of space used: here we mean the amount of extra space (system resources) beyond
the size of the input (independent of hardware and programming language);
we will say that an algorithm is {\em in place} if the amount of extra space
is constant with respect to input size

\item  Simplicity, clarity: unfortunately, the simplest is not always the
best in other ways

\item  Optimality: can we prove that it does as well as or better than any other algorithm?
\end{enumerate}

\goodbreak\section{Order of growth of algorithms}

\index{order or growth}\index{$O$}\index{$\Theta$}\index{$\Omega$}\index{$o$}\index{$\omega$}

\index{sort!insertion}

The {\it insertion sort} is a simple algorithm in which an array of elements is sorted in place, one entry at a time. It is not the fastest sorting algorithm, but it is simple and does not require extra memory other than the memory needed to store the input array.

The insertion sort works by iterating. Every iteration $i$ of the insertion sort removes one element from the input data and inserts it into the correct position in the already-sorted subarray {\ft A[j]} for $0 \le j < i$. The algorithm iterates $n$ times (where $n$ is the total size of the input array) until no input elements remain to be sorted:

\begin{lstlisting}
def insertion_sort(A):
    for i in xrange(1,len(A)):
        for j in xrange(i,0,-1):
            if A[j]<A[j-1]:
                A[j], A[j-1] = A[j-1], A[j]
            else: break
\end{lstlisting}

Here is an example:

\begin{lstlisting}
>>> import random
>>> a=[random.randint(0,100) for k in xrange(20)]
>>> insertion_sort(a)
>>> print(a)
[6, 8, 9, 17, 30, 31, 45, 48, 49, 56, 56, 57, 65, 66, 75, 75, 82, 89, 90, 99]
\end{lstlisting}

One important question is, how long does this algorithm take to run? How does its running time scale with the input size?

Given any algorithm, we can define three characteristic functions:

\begin{itemize}
\item  $T_{worst}(n)$: the running time in the worst case

\item  $T_{best}(n)$: the running time in the best case

\item  $T_{average}(n)$: the running time in the average case
\end{itemize}

The best case for an insertion sort is realized when the input is already sorted. In this case, the inner for loop exits (breaks) always at the first iteration, thus only the most outer loop is important, and this is proportional to $n$; therefore $T_{best}(n) \propto n$. The worst case for the insertion sort is realized when the input is sorted in reversed order. In this case, we can prove, and we do so subsequently, that $T_{worst}(n) \propto n^2$. For this algorithm, a statistical analysis shows that the worst case is also the average case.

Often we cannot determine exactly the running time function, but we may be able to set bounds to the running time.

We define the following sets:

\begin{itemize}
\item  $O(g(n))$: the set of functions that grow no faster than $g(n)$ when $%
n\rightarrow \infty $

\item  $\Omega (g(n))$: the set of functions that grow no slower than $g(n)$ when $%
n\rightarrow \infty $

\item  $\Theta (g(n))$: the set of functions that grow at the same rate as $g(n)$ when $%
n\rightarrow \infty $

\item  $o(g(n))$: the set of functions that grow slower than $g(n)$ when $n\rightarrow
\infty $

\item  $\omega (g(n))$: the set of functions that grow faster than $g(n)$ when $n\rightarrow \infty $
\end{itemize}

We can rewrite the preceding definitions in a more formal way:

\begin{eqnarray}
&&O(g(n))\equiv \left\{ f(n):\exists n_0,c_0,\ \forall n>n_0,\
0\leq f(n)<c_0g(n)\right\} \\
&&\Omega (g(n))\equiv \left\{ f(n):\exists n_0,c_0,\ \forall
n>n_0,\ 0\leq c_0g(n)<f(n)\right\} \\
&&\Theta (g(n))\equiv O(g(n))\cap \Omega (g(n)) \\
&&o(g(n))\equiv O(g(n))-\Omega (g(n)) \\
&&\omega (g(n))\equiv \Omega (g(n))-O(g(n))
\end{eqnarray}

We can also provide a practical rule to determine if a function $f$ belongs to one of the previous sets defined by $g$.

Compute the limit
\begin{equation}
\lim_{n\rightarrow \infty }\frac{f(n)}{g(n)}=a
\end{equation}
and look up the result in the following table:

\begin{equation}
\begin{tabular}{lll}
$a$ is positive or zero & $\Longrightarrow $ & $f(n)\in O(g(n))$ $%
\Leftrightarrow $ $f\preceq g$ \\
$a$ is positive or infinity & $\Longrightarrow $ & $f(n)\in \Omega (g(n))$ $%
\Leftrightarrow $ $f\succeq g$ \\
$a$ is positive & $\Longrightarrow $ & $f(n)\in \Theta (g(n))$ $%
\Leftrightarrow $ $f\sim g$ \\
$a$ is zero & $\Longrightarrow $ & $f(n)\in o(g(n))$ $\Leftrightarrow $ $%
f\prec g$ \\
$a$ is infinity & $\Longrightarrow $ & $f(n)\in \omega (g(n))$ $%
\Leftrightarrow $ $f\succ g$%
\end{tabular}
\end{equation}

Notice the preceding practical rule assumes the limits exist.

Here is an example:

Given $f(n) = n\log n+3n$ and $g(n) = n^2$
\begin{equation}
\lim_{n\rightarrow \infty }\frac{n\log n+3n}{n^2}\stackrel{l^{\prime }Hopital%
}{\longrightarrow }\lim_{n\rightarrow \infty }\frac{1/n}2=0
\end{equation}
we conclude that $n\log n+3n$ is in $O(n^2)$.



Given an algorithm $A$ that acts on input of size $n$, we say that the algorithm is $O(g(n))$ if its worst running time as a function of $n$ is in $O(g(n))$. Similarly, we say that the algorithm is in $\Omega(g(n))$ if its best running time is in $\Omega(g(n))$. We also say that the algorithm is in $ \Theta(g(n))$ if both its best running time and its worst running time are in  $\Theta(g(n))$.

More formally, we can write the following:

\begin{eqnarray}
T_{worst}(n) \in O(g(n)) &\Rightarrow& A \in O(g(n)) \\
T_{best}(n) \in \Omega(g(n)) &\Rightarrow& A \in \Omega(g(n)) \\
A \in O(g(n)) \textrm{and} A \in O(g(n))  &\Rightarrow& A \in \Theta(g(n)) \\
\end{eqnarray}

We still have not solved the problem of computing the best, average, and worst running times.

\goodbreak\subsection{Best and worst running times}

The procedure for computing the worst and best running times is similar. It is simple in theory but difficult in practice because it requires an understanding of the algorithm's inner workings.

Consider the following algorithm, which finds the minimum of an array or list A:
\begin{lstlisting}
def find_minimum(A):
    minimum = a[0]
    for element in A:
        if element < minimum:
            minimum = element
    return minimum
\end{lstlisting}


To compute the running time in the worst case, we assume that the maximum number of computations is performed. That happens when the if statements are always {\ft True}. To compute the best running time, we assume that the minimum number of computations is performed. That happens when the if statement is always {\ft False}.
Under each of the two scenarios, we compute the running time by counting how many times the most nested operation is performed.

In the preceding algorithm, the most nested operation is the evaluation of the {\ft if} statement, and that is executed for each element in {\ft A}; for example, assuming {\ft A} has $n$ elements, the {\ft if} statement will be executed $n$ times.

Therefore both the best and worst running times are proportional to $n$, thus making this algorithm $O(n)$, $\Omega(n)$, and $\Theta(n)$.

More formally, we can observe that this algorithm performs the following operations:

\begin{itemize}
\item  One assignment (line 2)

\item  Loops $n=${\ft len(A)} times (line 3)

\item  For each loop iteration, performs one comparison (line 4)

\item  Line 5 is executed only if the condition is true
\end{itemize}

Because there are no nested loops, the time to execute each loop iteration is about the same, and the running time is proportional to the number of loop iterations.

For a loop iteration that does not contain further loops, the time it takes to compute each iteration, its {\ft running time}, is constant (therefore equal to 1). For algorithms that contain nested loops, we will have to evaluate nested sums.

Here is the simplest example:

\begin{lstlisting}
def loop0(n):
    for i in xrange(0,n):
        print(i)
\end{lstlisting}

which we can map into

\begin{equation}
T(n)=\sum_{i=0}^{i<n}1=n\in \Theta (n)\Rightarrow \text{{\ft loop0}}\in
\Theta (n)
\end{equation}

Here is a similar example where we have a single loop (corresponding to a single sum) that loops $n^2$ times:

\begin{lstlisting}
def loop1(n):
    for i in xrange(0,n*n):
        print(i)
\end{lstlisting}

and here is the corresponding running time formula:

\begin{equation}
T(n)=\sum_{i=0}^{i<n^2}1=n^2\in \Theta (n^2)\Rightarrow \text{{\tt loop1}}%
\in \Theta (n^2)
\end{equation}

The following provides an example of nested loops:

\begin{lstlisting}
def loop2(n):
    for i in xrange(0,n):
        for j in xrange(0,n):
            print(i,j)
\end{lstlisting}

Here the time for the inner loop is directly determined by $n$ and does not depend on the outer loop's counter; therefore

\begin{equation}
T(n)=\sum_{i=0}^{i<n}\sum_{j=0}^{j<n}1=\sum_{i=0}^{i<n}n=n^2+...\in \Theta
(n^2)\Rightarrow \text{{\tt loop2}}\in \Theta (n^2)
\end{equation}

This is not always the case. In the following code, the inner loop does depend on the value of the outer loop:

\begin{lstlisting}
def loop3(n):
    for i in xrange(0,n):
        for j in xrange(0,i):
            print(i,j)
\end{lstlisting}

Therefore, when we write its running time in terms of a sum, care must be taken that the upper limit of the inner sum is the upper limit of the outer sum:

\begin{equation}
T(n)=\sum_{i=0}^{i<n}\sum_{j=0}^{j<i}1=\sum_{i=0}^{i<n}i=\frac 12n(n-1)\in
\Theta (n^2)\Rightarrow \text{{\tt loop3}}\in \Theta (n^2)
\end{equation}

The appendix of this book provides examples of typical sums that come up in these types of formulas and their solutions.

Here is one more example falling in the same category, although the inner loop depends quadratically on the index of the outer loop:

\subsubsection{Example: loop4}
\begin{lstlisting}
def loop4(n):
    for i in xrange(0,n):
        for j in xrange(0,i*i):
            print(i,j)
\end{lstlisting}

Therefore the formula for the running time is more complicated:

\begin{eqnarray}
T(n) &=&\sum_{i=0}^{i<n}\sum_{j=0}^{j<i^2}1=\sum_{i=0}^{i<n}i^2=\frac
16n(n-1)(2n-1)\in \Theta (n^3) \\
&\Rightarrow &\text{{\tt loop4}}\in \Theta (n^3)
\end{eqnarray}

If the algorithm does not contain nested loops, then we need to compute the running time of each loop and take the maximum:

\subsubsection{Example: concatenate0}
\begin{lstlisting}
def concatenate0(n):
    for i in xrange(n*n):
        print(i)
    for j in xrange(n*n*n):
        print(j)
\end{lstlisting}

\begin{equation}
T(n)=\Theta (\max (n^2,n^3))\Rightarrow \text{{\tt concatenate0}}\in \Theta
(n^3)
\end{equation}

If there is an if statement, we need to compute the running time for each condition and pick the maximum when computing the worst running time, or the minimum for the best running time:

\begin{lstlisting}
def concatenate1(n):
    if a<0:
        for i in xrange(n*n):
            print(i)
    else:
        for j in xrange(n*n*n):
            print(j)
\end{lstlisting}

\begin{equation}
T_{worst}(n)=\Theta (\max (n^2,n^3))\Rightarrow \text{{\tt concatenate1}}\in \O(n^3)
\end{equation}

\begin{equation}
T_{best}(n)=\Theta (\min (n^2,n^3))\Rightarrow \text{{\tt concatenate1}}\in \Omega(n^2)
\end{equation}

This can be expressed more formally as follows:

\begin{eqnarray}
O(f(n))+\Theta (g(n)) &=&\Theta (g(n))\text{ iff }f(n)\in O(g(n)) \\
\Theta (f(n))+\Theta (g(n)) &=&\Theta (g(n))\text{ iff }f(n)\in O(g(n)) \\
\Omega (f(n))+\Theta (g(n)) &=&\Omega (f(n))\text{ iff }f(n)\in \Omega (g(n))
\end{eqnarray}

which we can apply as in the following example:
\begin{equation}
T(n)=[\stackunder{\Theta (n^2)}{\underbrace{n^2+n+3}}+\stackunder{\Theta
(e^n)}{\underbrace{e^n-\log n}}]\in \Theta (e^n)\text{ because }n^2\in
O(e^n)
\end{equation}

\goodbreak\section{Recurrence relations}

\index{recurrence relations}\index{mergesort}

\index{sort!merge}

The {\it merge sort}~\cite{mergesort} is another sorting algorithm. It is faster than the insertion sort. It was invented by John von Neumann, the physicist credited for inventing also modern computer architecture and game theory.

The merge sort works as follows.

If the input array has length 0 or 1, then it is already sorted, and the algorithm does not perform any other operation.

If the input array has a length greater than 1, it divides the array into two subsets of about half the size. Each subarray is sorted by applying the merge sort recursively (it calls itself!). It then merges the two subarrays back into one sorted array (this step is called {\it merge}).

Consider the following Python implementation of the merge sort:

\begin{lstlisting}
def mergesort(A, p=0, r=None):
    if r is None: r = len(A)
    if p<r-1:
        q = int((p+r)/2)
        mergesort(A,p,q)
        mergesort(A,q,r)
        merge(A,p,q,r)

def merge(A,p,q,r):
    B,i,j = [],p,q
    while True:
        if A[i]<=A[j]:
            B.append(A[i])
            i=i+1
        else:
            B.append(A[j])
            j=j+1
        if i==q:
            while j<r:
                B.append(A[j])
                j=j+1
            break
        if j==r:
            while i<q:
                B.append(A[i])
                i=i+1
            break
    A[p:r]=B
\end{lstlisting}

\index{recursion}

Because this algorithm calls itself {\it recursively}, it is more difficult to compute its running time.

Consider the {\ft merge} function first. At each step, it increases either $i$ or $j$, where $i$ is always in between $p$ and $q$ and $j$ is always in between $q$ and $r$. This means that the running time of the merge is proportional to the total number of values they can span from $p$ to $r$. This implies that
\begin{equation}
\textrm{merge} \in \Theta(r-p)
\end{equation}

We cannot compute the running time of the {\ft mergesort} function using the same direct analysis, but we can assume its running time is $T(n)$, where $n=r-p$ and $n$ is the size of the input data to be sorted and also the difference between its two arguments $p$ and $r$. We can express this running time in terms of its components:
\begin{itemize}
\item It calls itself twice on half of the input data, $2T(n/2)$
\item It calls the merge once on the entire data, $\Theta(n)$
\end{itemize}

We can summarize this into

\begin{eqnarray}
T(n) = 2T(n/2) + n
\end{eqnarray}

This is called a {\it recurrence relation}. We turned the problem of computing the running time of the algorithm into the problem of solving the recurrence relation. This is now a math problem.

Some recurrence relations can be difficult to solve, but most of them follow in one of these categories:

\begin{eqnarray}
T(n) \!\!\!&=&\!\!\!aT(n-b)+\Theta (f(n))\Rightarrow T(n)\in \Theta (max(a^n,nf(n))) \\
T(n) \!\!\!&=&\!\!\!T(b)+T(n-b-a)+\Theta(f(n))\Rightarrow T(n)\in \Theta (nf(n)) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m)\text{ and }a<b^m\Rightarrow T(n)\in \Theta (n^m) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m)\text{ and }a=b^m\Rightarrow T(n)\in \Theta
(n^m\log n) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m)\text{ and }a>b^m\Rightarrow T(n)\in \Theta
(n^{\log _ba}) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m\log ^pn)\text{ and }a<b^m\Rightarrow T(n)\in
\Theta (n^m\log ^pn) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m\log ^pn)\text{ and }a=b^m\Rightarrow T(n)\in
\Theta (n^m\log ^{p+1}n) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (n^m\log ^pn)\text{ and }a>b^m\Rightarrow T(n)\in
\Theta (n^{\log _ba}) \\
T(n) \!\!\!&=&\!\!\!aT(n/b)+\Theta (q^n)\Rightarrow T(n)\in \Theta (q^n) \\
T(n) \!\!\!&=&\!\!\!aT(n/a-b)+\Theta (f(n)) \Rightarrow T(n)\in \Theta (f(n)\log(n))
\end{eqnarray}
(they work for $m\geq 0$, $p$ $\geq 0$, and $q>1$).

\index{master theorem}

These results are a practical simplification of a theorem known as the {\it master theorem}~\cite{mastertheorem}.

\goodbreak\subsection{Reducible recurrence relations}

Other recurrence relations do not immediately fit one of the preceding patterns, but often they can be reduced (transformed) to fit.

Consider the following recurrence relation:
\begin{equation}
T(n)=
2T(\sqrt{n})+\log n
\label{otherrr}
\end{equation}

We can replace $n$ with $e^k=n$ in eq. (\ref{otherrr}) and obtain
\begin{equation}
T(e^k)=
2T(e^{k/2})+k
\end{equation}
If we also replace $T(e^k)$ with $S(k)=T(e^k)$, we obtain
\begin{equation}
\stackunder{T(e^k)}{\underbrace{S(k)}}=
2\stackunder{T(e^{k/2})}{\underbrace{S(k/2)}}+k
\end{equation}
so that we can now apply the master theorem to $S$. We obtain that $S(k)\in
\Theta (k\log k)$. Once we have the order of growth of $S$, we can determine
the order of growth of $T(n)$ by substitution:

\begin{equation}
T(n)=S(\log n)\in \Theta (\stackunder{k}{\underbrace{\log n}}\log
\stackunder{k}{\underbrace{\log n}})
\end{equation}

Note that there are recurrence relations that cannot be solved with any of
the methods described.

Here are some examples of recursive algorithms and their corresponding recurrence relations with solution:

\begin{lstlisting}
def factorial1(n):
    if n==0:
        return 1
    else:
        return n*factorial1(n-1)
\end{lstlisting}

\begin{equation}
T(n)=T(n-1)+1\Rightarrow T(n)\in \Theta (n)\Rightarrow \text{{\tt factorial1}%
}\in \Theta (n)
\end{equation}

\begin{lstlisting}
def recursive0(n):
    if n==0:
        return 1
    else:
        loop3(n)
        return n*n*recursive0(n-1)
\end{lstlisting}

\begin{equation}
T(n)=T(n-1)+P_2(n)\Rightarrow T(n)\in \Theta (n^2)\Rightarrow \text{{\tt %
recursive0}}\in \Theta (n^3)
\end{equation}

\begin{lstlisting}
def recursive1(n):
    if n==0:
        return 1
    else:
        loop3(n)
        return n*recursive1(n-1)*recursive1(n-1)
\end{lstlisting}

\begin{equation}
T(n)=2T(n-1)+P_2(n)\Rightarrow T(n)\in \Theta (2^n)\Rightarrow \text{{\tt %
recursive1}}\in \Theta (2^n)
\end{equation}

\begin{lstlisting}
def recursive2(n):
    if n==0:
        return 1
    else:
        a=factorial0(n)
        return a*recursive2(n/2)*recursive1(n/2)
\end{lstlisting}

\begin{equation}
T(n)=2T(n/2)+P_1(n)\Rightarrow T(n)\in \Theta (n\log n)\Rightarrow \text{%
{\tt recursive2}}\in \Theta (n\log n)
\end{equation}

One example of practical interest for us is the binary search below. It finds the location of the element in a sorted input array $A$:

\begin{lstlisting}
def binary_search(A,element):
    a,b = 0, len(A)-1
    while b>=a:
        x = int((a+b)/2)
        if A[x]<element:
            a = x+1
        elif A[x]>element:
            b = x-1
        else:
            return x
    return None
\end{lstlisting}

Notice that this algorithm does not appear to be recursive, but in practice, it is because of the apparently infinite while loop. The content of the while loop runs in constant time and then loops again on a problem of half of the original size:

\begin{equation}
T(n)=T(n/2)+1\Rightarrow \text{{\tt binary\_search}}\in \Theta(\log n)
\end{equation}

The idea of the {\ft binary\_search} is used in the bisection method for solving nonlinear equations.

Do not confuse {\ft T} notation with $\Theta$ notation:

\begin{table}
\begin{tabular}{|l|l|l|}\hline
Algorithm & Recurrence Relationship & Running time \\ \hline
Binary Search & $T(n) = T(\frac n 2) + \Theta (1)$ & $\Theta(log(n))$ \\ \hline
Binary Tree Traversal & $T(n) = 2T(\frac n 2) + \Theta (1)$ & $\Theta(n)$ \\ \hline
Optimal Sorted Matrix Search & $T(n) = 2T(\frac n 2) + \Theta (log(n))$ & $\Theta(n)$ \\ \hline
Merge Sort & $T(n) = T(\frac n 2) + \Theta (n)$ & $\Theta(n log(n))$ \\ \hline
\end{tabular}
\label{table1}
\end{table}

The theta notation can also be used to describe the memory used by an algorithm as a function of the input, $T_{memory}$, as well as its running time.

\goodbreak\section{Types of algorithms}

\index{divide and conquer}

{\bf Divide-and-conquer}
\index{divide and conquer} is a method of designing algorithms that
(informally) proceeds as follows: given an instance of the problem to be
solved, split this into several, smaller sub-instances (of the same
problem), independently solve each of the sub-instances and then combine the
sub-instance solutions to yield a solution for the original instance.
This description raises the question, by what methods are the sub-instances
to be independently solved? The answer to this question is central to the
concept of the divide-and-conquer algorithm and is a key factor in gauging
their efficiency. The solution is unique for each problem.

The merge sort algorithm of the previous section is an example of a divide-and-conquer algorithm. In the merge sort, we sort an array by dividing it into two arrays and recursively sorting (conquering) each of the smaller arrays.

Most divide-and-conquer algorithms are recursive, although this is not a requirement.

{\bf Dynamic programming}
\index{dynamic programming}
is a paradigm that is most often applied in the
construction of algorithms to solve a certain class of optimization
problems, that is, problems that require the minimization or maximization of
some measure. One disadvantage of using divide-and-conquer is that the
process of recursively solving separate sub-instances can result in the same
computations being performed repeatedly because identical sub-instances may
arise. For example, if you are computing the path between two nodes in a graph,
some portions of multiple paths will follow the same last few hops. Why compute
the last few hops for every path when you would get the same result every time?

The idea behind dynamic programming is to avoid this pathology by
obviating the requirement to calculate the same quantity twice. The method
usually accomplishes this by maintaining a table of sub-instance results. We
say that dynamic programming is a bottom-up technique in which the smallest
sub-instances are explicitly solved first and the results of these are used to
construct solutions to progressively larger sub-instances. In contrast, we
say that the divide-and-conquer is a top-down technique.


We can refactor the {\ft mergesort} algorithm to eliminate recursion in the algorithm implementation, while keeping the logic of the algorithm unchanged. Here is a possible implementation:
\begin{lstlisting}
def mergesort_nonrecursive(A):
    blocksize, n = 1, len(A)
    while blocksize<n:
        for p in xrange(0, n, 2*blocksize):
            q = p+blocksize
            r = min(q+blocksize, n)
            if r>q:
                Merge(A,p,q,r)
        blocksize = 2*blocksize
\end{lstlisting}

Notice that this has the same running time as the original {\ft mergesort} because, although it is not recursive, it performs the same operations:

\begin{eqnarray}
T_{best} &\in &\Theta (n\log n) \\
T_{average} &\in &\Theta (n\log n) \\
T_{worst} &\in &\Theta (n\log n) \\
T_{memory} &\in &\Theta (1)
\end{eqnarray}

\index{greedy algorithms}

{\bf Greedy algorithms}
\index{greedy algorithms} work in phases. In each phase, a decision is made
that appears to be good, without regard for future consequences. Generally,
this means that some local optimum is chosen. This ``take what you can get
now'' strategy is the source of the name for this class of algorithms. When
the algorithm terminates, we hope that the local optimum is equal to the
global optimum. If this is the case, then the algorithm is correct;
otherwise, the algorithm has produced a suboptimal solution. If the best
answer is not required, then simple greedy algorithms are sometimes used to
generate approximate answers, rather than using the more complicated
algorithms generally required to generate an exact answer. Even for problems
that can be solved exactly by a greedy algorithm, establishing the
correctness of the method may be a nontrivial process.

For example, computing change for a purchase in a store is a good case of a greedy algorithm.  Assume you need to give change back for a purchase.  You would have three choices:
\begin{itemize}
\item Give the smallest denomination repeatedly until the correct amount is returned

\item Give a random denomination repeatedly until you reach the correct amount. If a random choice exceeds the total, then pick another denomination until the correct amount is returned

\item Give the largest denomination less than the amount to return repeatedly until the correct amount is returned
\end{itemize}

In this case, the third choice is the correct one.

Other types of algorithms do not fit into any of the preceding
categories. One is, for example, backtracking. Backtracking is not covered
in this course.

\goodbreak\subsection{Memoization}

\index{memoization}

One case of a top-down approach that is very general and falls under the umbrella of dynamic programming is called {\it memoization}. Memoization consists of allowing users to write algorithms using a naive divide-and-conquer approach, but functions that may be called more than once are modified so that their output is cached, and if they are called again with the same initial state, instead of the algorithm running again, the output is retrieved from the cache and returned without any computations.

\index{Fibonacci series}

Consider, for example, Fibonacci numbers:

\begin{eqnarray}
\textrm{Fib}(0)&=&0\\
\textrm{Fib}(1)&=&1\\
\textrm{Fib}(n)&=&\textrm{Fib}(n-1)+\textrm{Fib}(n-2)\textrm{ for }n>1
\end{eqnarray}

which we can implement using divide-and-conquer as follows:

\begin{lstlisting}
def fib(n):
    return n if n<2 else fib(n-1)+fib(n-2)
\end{lstlisting}

The recurrence relation for this algorithm is $T(n)=T(n-1)+T(n-2)+1$, and its solution can be proven to be exponential. This is because this algorithm calls itself more than necessary with the same input values and keeps solving the same subproblem over and over.

\index{class!Memoize}

Python can implement memoization using the following decorator:
%%% META:FILE:nlib.py
\begin{lstlisting}[caption={in file: {\ft nlib.py}}]
class memoize(object):
    def __init__ (self, f):
        self.f = f
        self.storage = {}
    def __call__ (self, *args, **kwargs):
        key = str((self.f.__name__, args, kwargs))
        try:
            value = self.storage[key]
        except KeyError:
            value = self.f(*args, **kwargs)
            self.storage[key] = value
        return value
\end{lstlisting}

and simply decorating the recursive function as follows:

%%% META:FILE:nlib.py
\begin{lstlisting}[caption={in file: {\ft nlib.py}}]
@memoize
def fib(n):
    return n if n<2 else fib(n-1)+fib(n-2)
\end{lstlisting}

which we can call as

%%% META:FILE:nlib.py
\begin{lstlisting}[caption={in file: {\ft nlib.py}}]
>>> print(fib(11))
89
\end{lstlisting}

A decorator is a Python function that takes a function and returns a callable object (or a function) to replace the one passed as input. In the previous example, we are using the {\ft @memoize} decorator to replace the {\ft fib} function with the {\ft \_\_call\_\_} argument of the {\ft memoize} class.

This makes the algorithm run much faster. Its running time goes from exponential to linear. Notice that the preceding {\ft memoize} decorator is very general and can be used to decorate any other function.

One more direct
dynamic programming approach consists in removing the recursion:

\begin{lstlisting}
def fib(n):
    if n < 2: return n
    a, b = 0, 1
    for i in xrange(1,n):
        a, b = b, a+b
    return b
\end{lstlisting}

This also makes the algorithm linear and $T(n) \in \Theta(n)$.

\index{memoize\_persistent}

Notice that we easily modify the memoization algorithm to store the partial results in a shared space, for example, on disk using the {\ft PersistentDictionary}:

%%% META:FILE:nlib.py
\begin{lstlisting}[caption={in file: {\ft nlib.py}}]
class memoize_persistent(object):
    STORAGE = 'memoize.sqlite'
    def __init__ (self, f):
        self.f = f
        self.storage = PersistentDictionary(memoize_persistent.STORAGE)
    def __call__ (self, *args, **kwargs):
        key = str((self.f.__name__, args, kwargs))
        try:
            value = self.storage[key]
        except KeyError:
            value = self.f(*args, **kwargs)
            self.storage[key] = value
        return value
\end{lstlisting}

We can use it as we did before, but we can now start and stop the program or run concurrent parallel programs, and as long as they have access to the ``memoize.sqlite'' file, they will share the cache.


\goodbreak\section{Timing algorithms}

The order of growth is a theoretical concept. In practice, we need to time algorithms to check if findings are correct and, more important, to determine the magnitude of the constants in the $T$ functions.

For example, consider this:
\begin{lstlisting}
def f1(n):
    return sum(g1(x) for x in range(n))

def f2(n):
    return sum(g2(x) for x in range(n**2))
\end{lstlisting}


Since {\ft f1} is $\Theta(n)$ and {\ft f2} is $\Theta(n^2)$, we may be led to conclude that the latter is slower. It may very well be that {\ft g1} is $10^6$ smaller than {\ft g2} and therefore $T_{f1}(n) = c_1 n$, $T_{f2}(n) = c_2 n^2$, but if $c_1 = 10^6 c_2$, then $T_{f1}(n) > T_{f2}(n)$ when $n<10^6$.

To time functions in Python, we can use this simple algorithm:

%%% META:FILE:nlib.py
\begin{lstlisting}
def timef(f, ns=1000, dt = 60):
    import time
    t = t0 = time.time()
    for k in xrange(1,ns):
        f()
        t = time.time()
        if t-t0>dt: break
    return (t-t0)/k
\end{lstlisting}

This function calls and averages the running time of {\ft f()} for the minimum between {\ft ns=1000} iterations and {\ft dt=60} seconds.

It is now easy, for example, to time the fib function without memoize,
\begin{lstlisting}
>>> def fib(n):
...     return n if n<2 else fib(n-1)+fib(n-2)
>>> for k in range(15,20):
...     print k,timef(lambda:fib(k))
15 0.000315684575338
16 0.000576375363706
17 0.000936052104732
18 0.00135168084153
19 0.00217730337912
\end{lstlisting}
and with memoize,
\begin{lstlisting}
>>> @memoize
... def fib(n):
...     return n if n<2 else fib(n-1)+fib(n-2)
>>> for k in range(15,20):
...     print k,timef(lambda:fib(k))
15 4.24022311802e-06
16 4.02901146386e-06
17 4.21922128122e-06
18 4.02495429084e-06
19 3.73784963552e-06
\end{lstlisting}

The former shows an exponential behavior; the latter does not.

\goodbreak\section{Data structures}

\goodbreak\subsection{Arrays}

An array is a data structure in which a series of numbers are stored contiguously in memory. The time to access each number (to read or write it) is constant. The time to remove, append, or insert an element may require moving the entire array to a more spacious memory location, and therefore, in the worst case, the time is proportional to the size of the array.

Arrays are the appropriate containers when the number of elements does not change often and when elements have to be accessed in random order.

\goodbreak\subsection{List}

A list is a data structure in which data are not stored contiguously, and each element has knowledge of the location of the next element (and perhaps of the previous element, in a doubly linked list). This means that accessing any element for (read and write) requires finding the element and therefore looping. In the worst case, the time to find an element is proportional to the size of the list. Once an element has been found, any operation on the element, including read, write, delete, and insert, before or after can be done in constant time.

Lists are the appropriate choice when the number of elements can vary often and when their elements are usually accessed sequentially via iterations.

In Python, what is called a {\ft list} is actually an array of pointers to the elements.

\goodbreak\subsection{Stack}

\index{stack}\index{push}\index{pop}

A stack data structure is a container, and it is usually implemented as a list. It has the property that the first thing you can take out is the last thing put in. This is commonly known as last-in, first-out, or LIFO. The method to insert or add data to the container is called {\it push}, and the method to extract data is called {\it pop}.

In Python, we can implement push by appending an item at the end of a list (Python already has a method for this called {\ft .append}), and we can implement pop by removing the last element of a list and returning it (Python has a method for this called {\ft .pop}).

A simple stack example is as follows:

\begin{lstlisting}
>>> stk = []
>>> stk.append("One")
>>> stk.append("Two")
>>> print stk.pop()
Two
>>> stk.append("Three")
>>> print stk.pop()
Three
>>> print stk.pop()
One
\end{lstlisting}


\goodbreak\subsection{Queue}

A queue data structure is similar to a stack but, whereas the stack returns the most recent item added, a queue returns the oldest item in the list.  This is commonly called first-in, first-out, or FIFO.  To use Python lists to implement a queue, insert the element to add in the first position of the list as follows:

\begin{lstlisting}
>>> que = []
>>> que.insert(0,"One")
>>> que.insert(0,"Two")
>>> print que.pop()
One
>>> que.insert(0,"Three")
>>> print que.pop()
Two
>>> print que.pop()
Three
\end{lstlisting}

Lists in Python are not an efficient mechanism for implementing queues.  Each insertion or removal of an element at the front of a list requires all the elements in the list to be shifted by one. The Python package {\ft collections.deque} is designed to implement queues and stacks.  For a stack or queue, you use the same method {\ft .append} to add items.  For a stack, {\ft .pop} is used to return the most recent item added, while to build a queue, use {\ft .popleft} to remove the oldest item in the list:

\begin{lstlisting}
>>> from collections import deque
>>> que = deque([])
>>> que.append("One")
>>> que.append("Two")
>>> print que.popleft()
One
>>> que.append("Three")
>>> print que.popleft()
Two
>>> print que.popleft()
Three
\end{lstlisting}


\goodbreak\subsection{Sorting}

\index{sort!quicksort}

In the previous sections, we have seen the {\it insertion sort} and the {\it merge sort}. Here we consider, as examples, other sorting algorithms: the {\it quicksort}~\cite{mergesort}, the {\it randomized quicksort}, and the {\it counting sort}:

\begin{lstlisting}
def quicksort(A,p=0,r=-1):
    if r is -1:
        r=len(A)
    if p<r-1:
        q=partition(A,p,r)
        quicksort(A,p,q)
        quicksort(A,q+1,r)

def partition(A,i,j):
    x=A[i]
    h=i
    for k in xrange(i+1,j):
        if A[k]<x:
            h=h+1
            A[h],A[k] = A[k],A[h]
    A[h],A[i] = A[i],A[h]
    return h
\end{lstlisting}

The running time of the quicksort is given by

\begin{eqnarray}
T_{best} &\in &\Theta (n\log n) \\
T_{average} &\in &\Theta (n\log n) \\
T_{worst} &\in &\Theta (n^2) \\
\end{eqnarray}

The quicksort can also be randomized by picking the pivot, {\ft A[r]}, at random:
\begin{lstlisting}
def quicksort(A,p=0,r=-1):
    if r is -1:
        r=len(A)
    if p<r-1:
        q = random.randint(p,r-1)
        A[p], A[q] = A[q], A[p]
        q=partition(A,p,r)
        quicksort(A,p,q)
        quicksort(A,q+1,r)
\end{lstlisting}

In this case, the best and the worst running times do not change, but the average improves when the input is already almost sorted.

\index{sort!countingsort}

The {\it counting sort} algorithm is special because it only works for arrays of positive integers. This extra requirement allows it to run faster than other sorting algorithms, under some conditions. In fact, this algorithm is linear in the range span by the elements of the input array.

Here is a possible implementation:
\begin{lstlisting}
def countingsort(A):
    if min(A)<0:
        raise '_counting_sort List Unbound'
    i, n, k = 0, len(A), max(A)+1
    C = [0]*k
    for j in xrange(n):
        C[A[j]] = C[A[j]]+1
    for j in xrange(k):
        while C[j]>0:
           (A[i], C[j], i) = (j, C[j]-1, i+1)
\end{lstlisting}

If we define $k=max(A)-min(A)+1$ and $n=len(A)$, we see
\begin{eqnarray}
T_{best} &\in &\Theta (k+n) \\
T_{average} &\in &\Theta (k+n) \\
T_{worst} &\in &\Theta (k+n) \\
T_{memory} &\in &\Theta (k)
\end{eqnarray}
Notice that here we have also computed $T_{memory}$, for example, the order of growth of memory (not of time) as a function of the input size. In fact, this algorithm differs from the previous ones because it requires a temporary array $C$.

\end{document}
